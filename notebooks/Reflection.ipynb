{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6c3b0db7",
   "metadata": {},
   "source": [
    "# **Reflection**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00da70f8",
   "metadata": {},
   "source": [
    "### Thành viên: Nguyễn Hữu Khánh Hưng - 23120271\n",
    "\n",
    "- **Khó khăn gặp phải:**  \n",
    "  - Giai đoạn thu thập dữ liệu: Việc crawl từ phongtro123.com gặp nhiều trở ngại như rate limit (bị chặn request sau một số lượng lớn truy cập), dữ liệu không đồng nhất (nhiều bài đăng thiếu tiện ích hoặc mô tả text lộn xộn), và phải xử lý pagination thủ công để thu thập đủ >20.000 records mà không bị block IP.  \n",
    "  - Preprocessing & EDA: Dữ liệu thô có nhiều missing values (đặc biệt ở tiện ích như máy giặt, gác lửng), giá và diện tích cần regex phức tạp để chuyển về float, đồng thời phát hiện outliers (giá phòng \"ảo\" quá cao/thấp do lỗi nhập liệu hoặc đàm phán).  \n",
    "  - Modeling & Tuning: Xây dựng baseline Linear Regression khá đơn giản, nhưng khi tinh chỉnh hyperparameter cho XGBoost và Linear Regression Optimized by ElasticNet (grid/random search với learning_rate, max_depth, n_estimators...) tốn rất nhiều thời gian và tài nguyên tính toán (máy cá nhân chạy chậm, phải thử nhiều lần để tránh overfitting/underfitting).  \n",
    "  - Merge code & README: Khi hợp nhất code từ 4 thành viên, gặp conflict ở một số notebook như `Pre_Processing.ipynb`, phải refactor thủ công để code sạch và chạy ổn định trên mọi máy. Viết README chi tiết cũng mất kha khá thời gian để đảm bảo hướng dẫn tái hiện chính xác.\n",
    "\n",
    "- **Bài học rút ra:**  \n",
    "  - Preprocessing và feature engineering chiếm phần lớn thời gian (khoảng 60–70%) nhưng là yếu tố quyết định chất lượng mô hình – dữ liệu sạch giúp XGBoost đạt R² cao hơn hẳn Linear Regression.  \n",
    "  - Hyperparameter tuning không chỉ là \"thử nhiều\" mà cần chiến lược (bắt đầu từ learning_rate thấp + early stopping, dùng random search trước grid search để tiết kiệm thời gian).  \n",
    "  - Làm việc nhóm hiệu quả nhờ GitHub (branching, pull request, peer review) giúp tránh mất dữ liệu/code và học hỏi lẫn nhau (tôi học được cách xử lý categorical từ bạn Phạm Quốc Khánh, và word-embedding từ bạn Vũ Trần Phúc).  \n",
    "  - README và documentation rõ ràng rất quan trọng – không chỉ để giảng viên chấm mà còn giúp chính mình tái hiện dự án sau này.\n",
    "\n",
    "- **Nếu có thêm thời gian:**  \n",
    "  - Scale dataset lớn hơn bằng cách crawl thêm từ nhatot.com hoặc chotot.com để tăng độ đa dạng dữ liệu và giảm bias.  \n",
    "  - Thử nghiệm ensemble (stacking XGBoost + CatBoost + LightGBM) hoặc thêm Deep learning (MLP với PyTorch) để đẩy R² lên cao hơn.  \n",
    "  - Tích hợp thêm nhiều feature địa lý hơn (khoảng cách đến trường đại học/metro bằng Google Maps API) và deploy mô hình thành web app đơn giản (Streamlit) để dự đoán giá realtime cho sinh viên.  \n",
    "  - Thực hiện A/B testing hoặc cross-validation nâng cao hơn để đánh giá độ ổn định mô hình trên các phân khúc giá khác nhau (phòng dưới 3 triệu vs trên 5 triệu)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff11945a",
   "metadata": {},
   "source": [
    "### Thành viên: Phạm Quốc Khánh - 23120283\n",
    "\n",
    "* **Khó khăn gặp phải:**\n",
    "    * **Thách thức về thu thập dữ liệu:** Việc viết code crawl data đối mặt với nhiều trở ngại kỹ thuật như cấu trúc HTML của các trang tin không đồng nhất (đặc biệt là phần mô tả tiện ích) và cơ chế chặn request (rate limit) từ server, đòi hỏi phải xử lý ngoại lệ (exception handling) liên tục để đảm bảo thu thập đủ dữ liệu mà không bị ngắt quãng.\n",
    "    * **Xử lý dữ liệu thiếu (Missing Values):** Khi phân tích các biến Categorical (như loại phòng, tiện ích), em gặp khó khăn lớn trong việc ra quyết định điền giá trị khuyết. Việc chọn phương pháp điền (Imputation) sao cho hợp lý về mặt thực tế mà không làm sai lệch phân phối dữ liệu gốc là một bài toán cân não.\n",
    "    * **Tinh chỉnh mô hình phức tạp:** Mô hình XGBoost tuy mạnh nhưng có quá nhiều siêu tham số (hyperparameters). Việc tìm ra bộ tham số tối ưu thông qua Grid Search tốn rất nhiều thời gian tính toán và tài nguyên phần cứng, đôi khi dẫn đến tình trạng máy bị treo hoặc quá nhiệt.\n",
    "\n",
    "* **Cách vượt qua:**\n",
    "    * **Tối ưu hóa mã nguồn:** Em đã chia nhỏ quy trình crawl thành các batch nhỏ và lưu trữ trung gian (checkpoint), giúp việc resume lại quá trình thu thập dễ dàng hơn khi gặp lỗi mạng hoặc bị chặn IP.\n",
    "    * **Phân tích kỹ lưỡng trước khi xử lý:** Đối với các missing values, em đã thực hiện vẽ biểu đồ phân phối để so sánh trước và sau khi điền dữ liệu, đồng thời tham khảo ý kiến nhóm để thống nhất các quy tắc business logic (ví dụ: nếu diện tích < 15m2 thì khả năng cao không có máy giặt riêng).\n",
    "    * **Chiến lược Tuning hợp lý:** Thay vì chạy Grid Search vét cạn ngay từ đầu, em chuyển sang dùng Randomized Search để khoanh vùng tham số tốt trước, sau đó mới tinh chỉnh chi tiết, giúp tiết kiệm đáng kể thời gian training.\n",
    "\n",
    "* **Điều học được:**\n",
    "    * **Kỹ năng Data Cleaning chuyên sâu:** Học được cách sử dụng Regex và các thư viện Pandas thành thạo để \"làm sạch\" những dữ liệu văn bản lộn xộn, biến chúng thành các đặc trưng (features) sạch sẽ cho mô hình.\n",
    "    * **Tư duy định hướng dữ liệu (Data-driven):** Qua việc trả lời câu hỏi nghiên cứu số 1 (Địa lý) và số 2 (Tiện ích), em học được cách dùng số liệu để chứng minh hoặc bác bỏ các giả định cảm tính về giá nhà, ví dụ như mức độ ảnh hưởng thực sự của vị trí so với nội thất.\n",
    "    * **Làm chủ thuật toán Boosting:** Hiểu sâu hơn về cơ chế hoạt động của Gradient Boosting và cách các tham số như learning_rate hay max_depth ảnh hưởng trực tiếp đến độ phức tạp của model.\n",
    "\n",
    "* **Điều bất ngờ:**\n",
    "    * **Sự thống trị của vị trí địa lý:** Khi phân tích câu hỏi số 1, em khá bất ngờ khi thấy biến số về khu vực/quận huyện có tác động áp đảo lên giá phòng, lấn át hầu hết các tiện ích nội thất khác – điều này khẳng định câu nói \"Location, Location, Location\" trong bất động sản.\n",
    "    * **Hiệu năng của XGBoost:** Khả năng xử lý dữ liệu phi tuyến tính của XGBoost tốt hơn nhiều so với kỳ vọng ban đầu của em, đặc biệt là khi so sánh với các model cơ bản, nó bắt được các pattern phức tạp của dữ liệu giá phòng rất tốt.\n",
    "\n",
    "* **Hiểu biết mới về Data Science:**\n",
    "    * **Garbage In, Garbage Out:** Em thấm thía rằng dù thuật toán có xịn đến đâu (như XGBoost), nếu dữ liệu đầu vào (đặc biệt là các biến Categorical và Missing values) không được xử lý kỹ lưỡng thì kết quả dự đoán cũng sẽ vô nghĩa. Giai đoạn làm sạch dữ liệu thực sự là \"linh hồn\" của dự án.\n",
    "    * **Vai trò của trực quan hóa:** Các con số thống kê khô khan chỉ thực sự có ý nghĩa và dễ thuyết phục người khác (như trong báo cáo cuối kỳ) khi chúng được kể lại thông qua các biểu đồ trực quan, sinh động."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "073c9f39",
   "metadata": {},
   "source": [
    "### Thành viên Châu Huỳnh Phúc - 23120329\n",
    "\n",
    "* **Khó khăn gặp phải:**\n",
    "    * **Thách thức về kỹ thuật dữ liệu:** Với vai trò chịu trách nhiệm chính về Processing và Feature Engineering, khó khăn lớn nhất là việc xử lý các dữ liệu thô từ thực tế (website phongtro123.com) thường không nhất quán, chứa nhiều giá trị thiếu và sai lệch. Việc biến đổi các đặc trưng thô thành các biến có ý nghĩa cho mô hình học máy đòi hỏi sự tỉ mỉ và logic cao.\n",
    "    * **Phân tích tương quan phức tạp:** Khi thực hiện ma trận tương quan, việc xác định được đâu là mối liên hệ thực sự có giá trị giữa hàng chục biến số để rút ra insight không hề dễ dàng, đòi hỏi phải quan sát dữ liệu dưới nhiều góc độ khác nhau.\n",
    "    * **Áp lực quản lý:** Việc vừa thực hiện chuyên môn, vừa phải quản lý tiến độ, kiểm soát deadline và báo cáo trạng thái dự án cho cả nhóm đòi hỏi khả năng bao quát và xử lý tình huống linh hoạt để đảm bảo không thành viên nào bị chậm tiến độ.\n",
    "\n",
    "* **Cách vượt qua:**\n",
    "    * **Quy trình hóa công việc:** Áp dụng nghiêm ngặt quy trình CRISP-DM để chia nhỏ các giai đoạn xử lý, giúp việc kiểm soát dữ liệu từ khâu làm sạch đến Feature Engineering trở nên có hệ thống hơn.\n",
    "    * **Tận dụng sức mạnh nhóm:** Sử dụng cơ chế Peer Review (kiểm tra chéo) để các thành viên khác thẩm định lại logic xử lý dữ liệu của mình, giúp phát hiện sớm các lỗi sai sót trong mã nguồn.\n",
    "    * **Công cụ quản lý hiện đại:** Sử dụng triệt để Google Docs, Notion, ... để cập nhật trạng thái hàng tuần và GitHub để quản lý các phiên bản code, giúp việc theo dõi tiến độ của các thành viên (như việc crawl data theo các khoảng ID) trở nên minh bạch và dễ dàng điều chỉnh.\n",
    "\n",
    "* **Điều học được:**\n",
    "    * **Kỹ năng Feature Engineering:** Học được cách trích xuất và tối ưu hóa các đặc trưng quan trọng từ dữ liệu phòng trọ để nâng cao hiệu suất cho các mô hình như XGBoost hay CatBoost mà nhóm đã sử dụng.\n",
    "    * **Tư duy quản trị dự án:** Hiểu được tầm quan trọng của việc lập kế hoạch chi tiết (Milestones) và cách duy trì động lực cho nhóm thông qua việc giao tiếp liên tục trên Zalo và Google Meet.\n",
    "    * **Phân tích chuyên sâu:** Biết cách sử dụng ma trận tương quan và các kỹ thuật thống kê để bóc tách quy luật của thị trường, thay vì chỉ nhìn vào các con số bề nổi.\n",
    "\n",
    "* **Điều bất ngờ:**\n",
    "    * **Sức mạnh của yếu tố thời gian:** Em thực sự bất ngờ khi phân tích câu hỏi số 5 về yếu tố mùa vụ. Sự biến động giá thuê phòng trọ tại TP.HCM theo các cột mốc thời gian trong năm có quy luật rõ ràng hơn nhiều so với dự đoán ban đầu của nhóm.\n",
    "    * **Sự phức tạp của dữ liệu thực:** Dữ liệu thực tế khác xa với các bộ dữ liệu mẫu trong giáo trình; nó đòi hỏi sự can thiệp rất lớn của con người trong khâu tiền xử lý mới có thể đưa vào mô hình hóa.\n",
    "\n",
    "* **Hiểu biết mới về Data Science:**\n",
    "    * **Dữ liệu là cốt lõi:** Em rằng trong một dự án Khoa học dữ liệu, giai đoạn Tiền xử lý và Feature Engineering chiếm đến 80% công sức nhưng cũng chính là yếu tố quyết định sự thành bại của mô hình, chứ không chỉ là việc chọn thuật toán phức tạp.\n",
    "    * **Tính liên ngành:** Data Science không chỉ là lập trình hay toán học, mà còn cần sự am hiểu về lĩnh vực (domain knowledge) – cụ thể trong đồ án này là thị trường bất động sản cho thuê – để có thể đặt ra các câu hỏi nghiên cứu sắc bén và thực tế."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f628d43f",
   "metadata": {},
   "source": [
    "### Thành viên: Vũ Trần Phúc - 23120333\n",
    "\n",
    "* **Khó khăn gặp phải:**\n",
    "    * Giai đoạn thu thập dữ liệu: Dữ liệu thu thập được từ web thường không đồng nhất, các trường thông tin mô tả rải rác đòi hỏi phải xử lý kỹ lưỡng để phục vụ cho sau này.\n",
    "    * Preprocessing & EDA: Việc xử lý giá trị ngoại lai cho các biến số định lượng (giá, diện tích) rất nan giải do phân phối dữ liệu bị lệch hẳn về một phía. Rất khó để xác định ngưỡng cắt bỏ chính xác mà không vô tình loại bỏ các căn hộ cao cấp \"thật\" (giá cao do vị trí mặt tiền).\n",
    "    * Modeling & Tuning: Mô hình CatBoost tuy mạnh về biến phân loại nhưng lại có quá nhiều tham số phức tạp (độ sâu cây, hệ số điều chuẩn...). Việc tinh chỉnh để mô hình không bị overfitting trên tập huấn luyện tốn nhiều thời gian thử nghiệm mà kết quả kiểm thử đôi khi chưa cải thiện như kỳ vọng.\n",
    "\n",
    "* **Bài học rút ra:**\n",
    "    * Hiểu sâu về ngữ cảnh dữ liệu (Domain Knowledge) quan trọng không kém kỹ thuật code. Việc trực tiếp trả lời các câu hỏi nghiên cứu về biến động giá và vị trí giúp tôi hiểu rõ đặc thù thị trường, từ đó hỗ trợ tốt hơn cho quá trình xây dựng mô hình.\n",
    "    * Làm việc nhóm hiệu quả qua GitHub giúp tôi rèn luyện tính cẩn thận khi giải quyết xung đột mã nguồn . Tôi học hỏi được tư duy viết code sạch (clean code) từ bạn Khánh Hưng và cách xử lý dữ liệu thiếu logic từ bạn Quốc Khánh.\n",
    "    * Tầm quan trọng của kể chuyện với dữ liệu : Một mô hình chạy tốt cần đi kèm với một báo cáo súc tích, dễ hiểu. Kỹ năng trình bày kết quả là cầu nối quan trọng để đưa kỹ thuật đến gần hơn với thực tế.\n",
    "\n",
    "\n",
    "* **Nếu có thêm thời gian:**\n",
    "    * Phân tích địa lý trực quan hơn bằng cách sử dụng thư viện bản đồ để vẽ bản đồ nhiệt (Heatmap) cho TP.HCM, giúp người xem dễ dàng nhận diện vùng giá nóng/lạnh thay vì chỉ nhìn số liệu thống kê.\n",
    "    * Xây dựng hệ thống gợi ý bên cạnh bài toán dự đoán giá, giúp đề xuất các phòng trọ phù hợp dựa trên độ tương đồng về nhu cầu của người dùng."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef85e3d5",
   "metadata": {},
   "source": [
    "\n",
    "* **Nếu có thêm thời gian:**\n",
    "    * Cải thiện bộ crawl data để chạy đa luồng (multithreading), giúp thu thập dữ liệu nhanh hơn và mở rộng sang các trang web khác để so sánh.\n",
    "    * Thử nghiệm các kỹ thuật Imputation nâng cao hơn (như dùng KNN hoặc MICE) để xử lý missing values thay vì các phương pháp thống kê cơ bản.\n",
    "    * Đào sâu hơn vào Feature Importance của XGBoost để loại bỏ các biến nhiễu, giúp mô hình nhẹ hơn nhưng vẫn giữ được độ chính xác cao."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
