{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6c3b0db7",
   "metadata": {},
   "source": [
    "# **Reflection**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fac5cf1",
   "metadata": {},
   "source": [
    "## **Đối với từng thành viên**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00da70f8",
   "metadata": {},
   "source": [
    "### Thành viên: Nguyễn Hữu Khánh Hưng - 23120271\n",
    "\n",
    "- **Khó khăn gặp phải:**  \n",
    "  - Trong giai đoạn thu thập dữ liệu: Crawl từ phongtro123.com bị rate limit nghiêm trọng (bị chặn IP sau 200–300 request), dữ liệu không đồng nhất (nhiều bài đăng thiếu tiện ích, giá/diện tích ghi kiểu chữ như \"2tr5\", \"1.8x3.5m\"), và phải xử lý pagination thủ công để thu thập đủ batch lớn.  \n",
    "  - Preprocessing & EDA: Dữ liệu thô có tỷ lệ missing values cao (đặc biệt tiện ích như máy giặt, gác lửng), outliers giá cực đoan (phòng 50 triệu/tháng hoặc 500k), và text mô tả lộn xộn cần regex phức tạp để trích xuất thông tin.  \n",
    "  - Phần bonus Amenities Verification: Crawl ảnh gặp lỗi 403/timeout, Gemini API quota free tier hết rất nhanh (chỉ 5–10 request liên tục là 429 error), và prompt ban đầu không chính xác dẫn đến Gemini trả JSON sai format hoặc đoán mò tiện ích.  \n",
    "  - Modeling & Tuning: Hyperparameter tuning cho XGBoost và ElasticNet (GridSearchCV/RandomizedSearchCV) tốn rất nhiều thời gian và tài nguyên (máy cá nhân chỉ CPU nên chạy chậm).  \n",
    "  - Merge code & README: Khi hợp nhất code từ 4 thành viên, gặp conflict ở notebook Pre_Processing và Modeling (tên biến khác nhau, import trùng lặp), phải refactor thủ công; viết README chi tiết cũng mất thời gian để đảm bảo hướng dẫn tái hiện chính xác.\n",
    "\n",
    "- **Cách vượt qua:**  \n",
    "  - Crawl dữ liệu: Chia nhỏ batch (300 trang/lần), thêm random delay (time.sleep(1.5–3s)) và rotate User-Agent để tránh bị block; lưu từng file Page*.csv riêng để dễ resume nếu lỗi.  \n",
    "  - Preprocessing: Sử dụng regex kết hợp pandas apply để xử lý giá/diện tích, fill missing bằng mode/median theo quận, loại outliers bằng IQR method.  \n",
    "  - Amenities Verification: Resize ảnh xuống 640x640 để giảm token, thêm retry logic (thử lại 3 lần nếu 429), và tinh chỉnh prompt nghiêm ngặt hơn (\"chỉ count present nếu thấy rõ ràng, không đoán mò\"). Nếu hết quota, chuyển sang test nhỏ (5–10 bài đăng) trước.  \n",
    "  - Tuning mô hình: Chuyển sang RandomizedSearchCV thay GridSearchCV để giảm thời gian, dùng early stopping trong XGBoost, chạy trên Google Colab khi máy yếu.  \n",
    "  - Merge code: Sử dụng Git branching (feature/Pre_Processing, feature/Modeling), pull request + review trước khi merge; refactor code thành module trong utilities/ để dễ bảo trì.\n",
    "\n",
    "- **Điều học được:**  \n",
    "  - Preprocessing và feature engineering chiếm phần lớn công sức (60–70%) nhưng ảnh hưởng đến việc EDA cũng như Modeling sau này.\n",
    "  - Hyperparameter tuning cần chiến lược khoa học (bắt đầu learning_rate thấp + early stopping, dùng RandomizedSearchCV trước, sau đó refine bằng GridSearch nhỏ) thay vì thử ngẫu nhiên.  \n",
    "  - Prompt engineering rất quan trọng khi dùng LLM như Gemini – một prompt chi tiết, có quy tắc rõ ràng (ví dụ: \"không đoán mò, chỉ count nếu thấy rõ ràng\") giúp output JSON chính xác hơn 80%.  \n",
    "  - Làm việc nhóm hiệu quả nhờ GitHub branching, peer review, và công cụ như Zalo/Google Meet giúp phát hiện lỗi sớm và học hỏi lẫn nhau.\n",
    "\n",
    "- **Điều bất ngờ:**  \n",
    "  - Bất ngờ lớn nhất là phần bonus Amenities Verification lại hữu ích hơn dự kiến: tỷ lệ khai khống tiện ích thực tế cao, giúp nhận ra rằng dữ liệu \"tiện ích claim\" trong raw.csv có noise lớn, ảnh hưởng trực tiếp đến mô hình dự đoán giá.  \n",
    "  - Bất ngờ thứ hai là XGBoost sau tuning chỉ cải thiện R² lên 0.55 – thấp hơn kỳ vọng ban đầu (dự đoán 0.7+), chứng tỏ dữ liệu phòng trọ tại Việt Nam có noise rất cao (giá đàm phán, yếu tố chủ quan), và cần thêm feature địa lý hoặc NLP sâu để cải thiện.\n",
    "\n",
    "- **Hiểu biết mới về data science:**  \n",
    "  - Data science không chỉ là xây mô hình đẹp mà là toàn bộ pipeline: từ crawl dữ liệu thực tế (khó khăn hơn tưởng tượng), làm sạch noise lớn, đến đánh giá mô hình một cách khách quan (không chỉ nhìn R² mà phải xem MAE/RMSE trong ngữ cảnh thực tế).\n",
    "  - Multimodal AI (vision + text như Gemini) là xu hướng mạnh, có thể giải quyết vấn đề mà model truyền thống không làm được (kiểm tra hình ảnh tiện ích).  \n",
    "  - Làm việc nhóm trong data science đòi hỏi giao tiếp liên tục và version control nghiêm ngặt – GitHub không chỉ lưu code mà còn là công cụ minh chứng đóng góp cá nhân.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff11945a",
   "metadata": {},
   "source": [
    "### Thành viên: Phạm Quốc Khánh - 23120283\n",
    "\n",
    "* **Khó khăn gặp phải:**\n",
    "    * **Thách thức về thu thập dữ liệu:** Việc viết code crawl data đối mặt với nhiều trở ngại kỹ thuật như cấu trúc HTML của các trang tin không đồng nhất (đặc biệt là phần mô tả tiện ích) và cơ chế chặn request (rate limit) từ server, đòi hỏi phải xử lý ngoại lệ (exception handling) liên tục để đảm bảo thu thập đủ dữ liệu mà không bị ngắt quãng.\n",
    "    * **Xử lý dữ liệu thiếu (Missing Values):** Khi phân tích các biến Categorical (như loại phòng, tiện ích), em gặp khó khăn lớn trong việc ra quyết định điền giá trị khuyết. Việc chọn phương pháp điền (Imputation) sao cho hợp lý về mặt thực tế mà không làm sai lệch phân phối dữ liệu gốc là một bài toán cân não.\n",
    "    * **Tinh chỉnh mô hình phức tạp:** Mô hình XGBoost tuy mạnh nhưng có quá nhiều siêu tham số (hyperparameters). Việc tìm ra bộ tham số tối ưu thông qua Grid Search tốn rất nhiều thời gian tính toán và tài nguyên phần cứng, đôi khi dẫn đến tình trạng máy bị treo hoặc quá nhiệt.\n",
    "\n",
    "* **Cách vượt qua:**\n",
    "    * **Tối ưu hóa mã nguồn:** Em đã chia nhỏ quy trình crawl thành các batch nhỏ và lưu trữ trung gian (checkpoint), giúp việc resume lại quá trình thu thập dễ dàng hơn khi gặp lỗi mạng hoặc bị chặn IP.\n",
    "    * **Phân tích kỹ lưỡng trước khi xử lý:** Đối với các missing values, em đã thực hiện vẽ biểu đồ phân phối để so sánh trước và sau khi điền dữ liệu, đồng thời tham khảo ý kiến nhóm để thống nhất các quy tắc business logic (ví dụ: nếu diện tích < 15m2 thì khả năng cao không có máy giặt riêng).\n",
    "    * **Chiến lược Tuning hợp lý:** Thay vì chạy Grid Search vét cạn ngay từ đầu, em chuyển sang dùng Randomized Search để khoanh vùng tham số tốt trước, sau đó mới tinh chỉnh chi tiết, giúp tiết kiệm đáng kể thời gian training.\n",
    "\n",
    "* **Điều học được:**\n",
    "    * **Kỹ năng Data Cleaning chuyên sâu:** Học được cách sử dụng Regex và các thư viện Pandas thành thạo để \"làm sạch\" những dữ liệu văn bản lộn xộn, biến chúng thành các đặc trưng (features) sạch sẽ cho mô hình.\n",
    "    * **Tư duy định hướng dữ liệu (Data-driven):** Qua việc trả lời câu hỏi nghiên cứu số 1 (Địa lý) và số 2 (Tiện ích), em học được cách dùng số liệu để chứng minh hoặc bác bỏ các giả định cảm tính về giá nhà, ví dụ như mức độ ảnh hưởng thực sự của vị trí so với nội thất.\n",
    "    * **Làm chủ thuật toán Boosting:** Hiểu sâu hơn về cơ chế hoạt động của Gradient Boosting và cách các tham số như learning_rate hay max_depth ảnh hưởng trực tiếp đến độ phức tạp của model.\n",
    "\n",
    "* **Điều bất ngờ:**\n",
    "    * **Sự thống trị của vị trí địa lý:** Khi phân tích câu hỏi số 1, em khá bất ngờ khi thấy biến số về khu vực/quận huyện có tác động áp đảo lên giá phòng, lấn át hầu hết các tiện ích nội thất khác – điều này khẳng định câu nói \"Location, Location, Location\" trong bất động sản.\n",
    "    * **Hiệu năng của XGBoost:** Khả năng xử lý dữ liệu phi tuyến tính của XGBoost tốt hơn nhiều so với kỳ vọng ban đầu của em, đặc biệt là khi so sánh với các model cơ bản, nó bắt được các pattern phức tạp của dữ liệu giá phòng rất tốt.\n",
    "\n",
    "* **Hiểu biết mới về Data Science:**\n",
    "    * **Garbage In, Garbage Out:** Em thấm thía rằng dù thuật toán có xịn đến đâu (như XGBoost), nếu dữ liệu đầu vào (đặc biệt là các biến Categorical và Missing values) không được xử lý kỹ lưỡng thì kết quả dự đoán cũng sẽ vô nghĩa. Giai đoạn làm sạch dữ liệu thực sự là \"linh hồn\" của dự án.\n",
    "    * **Vai trò của trực quan hóa:** Các con số thống kê khô khan chỉ thực sự có ý nghĩa và dễ thuyết phục người khác (như trong báo cáo cuối kỳ) khi chúng được kể lại thông qua các biểu đồ trực quan, sinh động."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "073c9f39",
   "metadata": {},
   "source": [
    "### Thành viên Châu Huỳnh Phúc - 23120329\n",
    "\n",
    "* **Khó khăn gặp phải:**\n",
    "    * **Thách thức về kỹ thuật dữ liệu:** Với vai trò chịu trách nhiệm chính về Processing và Feature Engineering, khó khăn lớn nhất là việc xử lý các dữ liệu thô từ thực tế (website phongtro123.com) thường không nhất quán, chứa nhiều giá trị thiếu và sai lệch. Việc biến đổi các đặc trưng thô thành các biến có ý nghĩa cho mô hình học máy đòi hỏi sự tỉ mỉ và logic cao.\n",
    "    * **Phân tích tương quan phức tạp:** Khi thực hiện ma trận tương quan, việc xác định được đâu là mối liên hệ thực sự có giá trị giữa hàng chục biến số để rút ra insight không hề dễ dàng, đòi hỏi phải quan sát dữ liệu dưới nhiều góc độ khác nhau.\n",
    "    * **Áp lực quản lý:** Việc vừa thực hiện chuyên môn, vừa phải quản lý tiến độ, kiểm soát deadline và báo cáo trạng thái dự án cho cả nhóm đòi hỏi khả năng bao quát và xử lý tình huống linh hoạt để đảm bảo không thành viên nào bị chậm tiến độ.\n",
    "\n",
    "* **Cách vượt qua:**\n",
    "    * **Quy trình hóa công việc:** Áp dụng nghiêm ngặt quy trình CRISP-DM để chia nhỏ các giai đoạn xử lý, giúp việc kiểm soát dữ liệu từ khâu làm sạch đến Feature Engineering trở nên có hệ thống hơn.\n",
    "    * **Tận dụng sức mạnh nhóm:** Sử dụng cơ chế Peer Review (kiểm tra chéo) để các thành viên khác thẩm định lại logic xử lý dữ liệu của mình, giúp phát hiện sớm các lỗi sai sót trong mã nguồn.\n",
    "    * **Công cụ quản lý hiện đại:** Sử dụng triệt để Google Docs, Notion, ... để cập nhật trạng thái hàng tuần và GitHub để quản lý các phiên bản code, giúp việc theo dõi tiến độ của các thành viên (như việc crawl data theo các khoảng ID) trở nên minh bạch và dễ dàng điều chỉnh.\n",
    "\n",
    "* **Điều học được:**\n",
    "    * **Kỹ năng Feature Engineering:** Học được cách trích xuất và tối ưu hóa các đặc trưng quan trọng từ dữ liệu phòng trọ để nâng cao hiệu suất cho các mô hình như XGBoost hay CatBoost mà nhóm đã sử dụng.\n",
    "    * **Tư duy quản trị dự án:** Hiểu được tầm quan trọng của việc lập kế hoạch chi tiết (Milestones) và cách duy trì động lực cho nhóm thông qua việc giao tiếp liên tục trên Zalo và Google Meet.\n",
    "    * **Phân tích chuyên sâu:** Biết cách sử dụng ma trận tương quan và các kỹ thuật thống kê để bóc tách quy luật của thị trường, thay vì chỉ nhìn vào các con số bề nổi.\n",
    "\n",
    "* **Điều bất ngờ:**\n",
    "    * **Sức mạnh của yếu tố thời gian:** Em thực sự bất ngờ khi phân tích câu hỏi số 5 về yếu tố mùa vụ. Sự biến động giá thuê phòng trọ tại TP.HCM theo các cột mốc thời gian trong năm có quy luật rõ ràng hơn nhiều so với dự đoán ban đầu của nhóm.\n",
    "    * **Sự phức tạp của dữ liệu thực:** Dữ liệu thực tế khác xa với các bộ dữ liệu mẫu trong giáo trình; nó đòi hỏi sự can thiệp rất lớn của con người trong khâu tiền xử lý mới có thể đưa vào mô hình hóa.\n",
    "\n",
    "* **Hiểu biết mới về Data Science:**\n",
    "    * **Dữ liệu là cốt lõi:** Em rằng trong một dự án Khoa học dữ liệu, giai đoạn Tiền xử lý và Feature Engineering chiếm đến 80% công sức nhưng cũng chính là yếu tố quyết định sự thành bại của mô hình, chứ không chỉ là việc chọn thuật toán phức tạp.\n",
    "    * **Tính liên ngành:** Data Science không chỉ là lập trình hay toán học, mà còn cần sự am hiểu về lĩnh vực (domain knowledge) – cụ thể trong đồ án này là thị trường bất động sản cho thuê – để có thể đặt ra các câu hỏi nghiên cứu sắc bén và thực tế."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f628d43f",
   "metadata": {},
   "source": [
    "### Thành viên Vũ Trần Phúc – 23120333\n",
    "\n",
    "* **Khó khăn gặp phải:**\n",
    "\n",
    "  * **Biểu diễn ngữ nghĩa cho dữ liệu mô tả:** Thách thức lớn trong quá trình làm việc với dữ liệu văn bản là việc lựa chọn và đánh giá cách biểu diễn ngữ nghĩa phù hợp cho các mô tả phòng trọ. Không phải mọi thông tin trong văn bản đều đóng góp tích cực cho bài toán dự đoán giá, nên cần cân nhắc giữa việc giữ lại nội dung quan trọng và tránh làm tăng nhiễu cho mô hình.\n",
    "  * **Phân tích Numerical và Outliers:** Dữ liệu số như giá thuê, diện tích, số lượng tiện ích có phân phối lệch và xuất hiện nhiều outliers bất thường. Việc xác định đâu là outliers hợp lý về mặt thực tế thị trường, đâu là lỗi dữ liệu không đơn thuần chỉ dựa vào thống kê mà cần kết hợp với hiểu biết domain.\n",
    "  * **Diễn giải câu hỏi nghiên cứu:** Khi trả lời các câu hỏi nghiên cứu về mối quan hệ giữa biến động giá và vị trí (Câu 3, 4), thách thức nằm ở việc chuyển các kết quả phân tích định lượng thành các lập luận logic, dễ hiểu và có giá trị thực tiễn.\n",
    "\n",
    "* **Cách vượt qua:**\n",
    "\n",
    "  * **Thử nghiệm và đánh giá nhiều phương án embedding:** Tiến hành so sánh các cách biểu diễn văn bản khác nhau  từ đó lựa chọn phương án Word Embedding mang lại hiệu quả tốt nhất và có ảnh hưởng tích cực đến các chỉ số đánh giá.\n",
    "  * **Kết hợp thống kê và trực quan hóa:** Sử dụng boxplot, IQR và phân phối histogram để phát hiện outliers, đồng thời đối chiếu với bối cảnh thực tế của thị trường thuê phòng nhằm tránh loại bỏ các giá trị có ý nghĩa.\n",
    "  * **Phối hợp nhóm để kiểm chứng kết luận:** Trao đổi với các thành viên khác về cách diễn giải kết quả phân tích vị trí và giá, từ đó tinh chỉnh lập luận và đảm bảo tính nhất quán trong toàn bộ báo cáo.\n",
    "\n",
    "* **Điều học được:**\n",
    "\n",
    "  * **Ứng dụng Word Embedding trong bài toán thực tế:** Hiểu rõ cách biến dữ liệu văn bản phi cấu trúc thành các đặc trưng số có thể sử dụng hiệu quả trong mô hình học máy.\n",
    "  * **Phân tích dữ liệu Numerical chuyên sâu:** Nâng cao kỹ năng nhận diện và xử lý outliers, cũng như đánh giá ảnh hưởng của chúng đến kết quả mô hình và phân tích.\n",
    "  * **Xây dựng mô hình CatBoost:** Học được cách cấu hình, huấn luyện và đánh giá mô hình CatBoost cho dữ liệu hỗn hợp (Numerical + Categorical), cũng như vai trò của tuning tham số đối với hiệu suất mô hình.\n",
    "\n",
    "* **Điều bất ngờ:**\n",
    "\n",
    "  * **Tác động mạnh của vị trí đến giá thuê:** Khi phân tích câu hỏi nghiên cứu, em bất ngờ trước mức độ ảnh hưởng rõ rệt của yếu tố vị trí (quận, khu vực) đến biến động giá, đôi khi còn lớn hơn cả các đặc trưng vật lý như diện tích hay số tiện ích.\n",
    "  * **Giá trị của dữ liệu văn bản:** Các đặc trưng trích xuất từ mô tả phòng trọ sau khi embedding mang lại nhiều thông tin hơn dự đoán ban đầu và góp phần cải thiện kết quả mô hình.\n",
    "\n",
    "* **Hiểu biết mới về Data Science:**\n",
    "\n",
    "  * **Sự kết hợp đa dạng dữ liệu:** Một mô hình hiệu quả không chỉ dựa vào dữ liệu số mà còn cần khai thác tốt dữ liệu phi cấu trúc như văn bản để phản ánh đầy đủ thực tế.\n",
    "  * **Tầm quan trọng của diễn giải kết quả:** Data Science không dừng lại ở việc xây dựng mô hình có độ chính xác cao, mà còn ở khả năng giải thích kết quả và liên hệ chúng với các câu hỏi nghiên cứu và bối cảnh thực tế, đặc biệt trong báo cáo tổng kết dự án."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef85e3d5",
   "metadata": {},
   "source": [
    "## **Đối với Nhóm**\n",
    "**Nếu có thêm thời gian, nhóm sẽ tập trung:**\n",
    "\n",
    "* Thử nghiệm kỹ thuật Stacking Ensemble để kết hợp ưu điểm của nhiều thuật toán.\n",
    "* Cải thiện bộ crawl data để chạy đa luồng (multithreading), giúp thu thập dữ liệu nhanh hơn và mở rộng sang các trang web khác để so sánh.\n",
    "* Thử nghiệm các kỹ thuật Imputation nâng cao hơn (như dùng KNN hoặc MICE) để xử lý missing values thay vì các phương pháp thống kê cơ bản.\n",
    "* Đào sâu hơn vào Feature Importance của XGBoost để loại bỏ các biến nhiễu, giúp mô hình nhẹ hơn nhưng vẫn giữ được độ chính xác cao."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
