{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6c3b0db7",
   "metadata": {},
   "source": [
    "# **Reflection**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00da70f8",
   "metadata": {},
   "source": [
    "### Thành viên: Nguyễn Hữu Khánh Hưng - 23120271\n",
    "\n",
    "- **Khó khăn gặp phải:**  \n",
    "  - Giai đoạn thu thập dữ liệu: Việc crawl từ phongtro123.com gặp nhiều trở ngại như rate limit (bị chặn request sau một số lượng lớn truy cập), dữ liệu không đồng nhất (nhiều bài đăng thiếu tiện ích hoặc mô tả text lộn xộn), và phải xử lý pagination thủ công để thu thập đủ >20.000 records mà không bị block IP.  \n",
    "  - Preprocessing & EDA: Dữ liệu thô có nhiều missing values (đặc biệt ở tiện ích như máy giặt, gác lửng), giá và diện tích cần regex phức tạp để chuyển về float, đồng thời phát hiện outliers (giá phòng \"ảo\" quá cao/thấp do lỗi nhập liệu hoặc đàm phán).  \n",
    "  - Modeling & Tuning: Xây dựng baseline Linear Regression khá đơn giản, nhưng khi tinh chỉnh hyperparameter cho XGBoost và Linear Regression Optimized by ElasticNet (grid/random search với learning_rate, max_depth, n_estimators...) tốn rất nhiều thời gian và tài nguyên tính toán (máy cá nhân chạy chậm, phải thử nhiều lần để tránh overfitting/underfitting).  \n",
    "  - Merge code & README: Khi hợp nhất code từ 4 thành viên, gặp conflict ở một số notebook như `Pre_Processing.ipynb`, phải refactor thủ công để code sạch và chạy ổn định trên mọi máy. Viết README chi tiết cũng mất kha khá thời gian để đảm bảo hướng dẫn tái hiện chính xác.\n",
    "\n",
    "- **Bài học rút ra:**  \n",
    "  - Preprocessing và feature engineering chiếm phần lớn thời gian (khoảng 60–70%) nhưng là yếu tố quyết định chất lượng mô hình – dữ liệu sạch giúp XGBoost đạt R² cao hơn hẳn Linear Regression.  \n",
    "  - Hyperparameter tuning không chỉ là \"thử nhiều\" mà cần chiến lược (bắt đầu từ learning_rate thấp + early stopping, dùng random search trước grid search để tiết kiệm thời gian).  \n",
    "  - Làm việc nhóm hiệu quả nhờ GitHub (branching, pull request, peer review) giúp tránh mất dữ liệu/code và học hỏi lẫn nhau (tôi học được cách xử lý categorical từ bạn Phạm Quốc Khánh, và word-embedding từ bạn Vũ Trần Phúc).  \n",
    "  - README và documentation rõ ràng rất quan trọng – không chỉ để giảng viên chấm mà còn giúp chính mình tái hiện dự án sau này.\n",
    "\n",
    "- **Nếu có thêm thời gian:**  \n",
    "  - Scale dataset lớn hơn bằng cách crawl thêm từ nhatot.com hoặc chotot.com để tăng độ đa dạng dữ liệu và giảm bias.  \n",
    "  - Thử nghiệm ensemble (stacking XGBoost + CatBoost + LightGBM) hoặc thêm Deep learning (MLP với PyTorch) để đẩy R² lên cao hơn.  \n",
    "  - Tích hợp thêm nhiều feature địa lý hơn (khoảng cách đến trường đại học/metro bằng Google Maps API) và deploy mô hình thành web app đơn giản (Streamlit) để dự đoán giá realtime cho sinh viên.  \n",
    "  - Thực hiện A/B testing hoặc cross-validation nâng cao hơn để đánh giá độ ổn định mô hình trên các phân khúc giá khác nhau (phòng dưới 3 triệu vs trên 5 triệu)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82c79bb4",
   "metadata": {},
   "source": [
    "### Thành viên: Phạm Quốc Khánh - 23120283\n",
    "\n",
    "* **Khó khăn gặp phải:**\n",
    "    * **Thu thập dữ liệu (Crawling):** Đây là thách thức đầu tiên khi cấu trúc HTML của các trang chi tiết không hoàn toàn đồng nhất, dẫn đến việc code crawl bị lỗi khi gặp các trường hợp ngoại lệ (edge cases). Ngoài ra, việc xử lý cơ chế chặn (rate limit) của server và đảm bảo crawl đủ số lượng bản ghi mà không bị ngắt quãng đòi hỏi phải liên tục theo dõi và debug.\n",
    "    * **Xử lý dữ liệu (Categorical & Missing):** Các biến phân loại (Categorical) như \"Tiện ích\" hay \"Khu vực\" chứa nhiều dữ liệu rác và không chuẩn hóa (ví dụ: nhiều cách viết khác nhau cho cùng một địa danh). Việc quyết định phương pháp điền dữ liệu khuyết (Missing values) sao cho không làm lệch phân phối dữ liệu gốc cũng tốn nhiều thời gian thử nghiệm và đánh giá.\n",
    "    * **Mô hình hóa (Modeling):** Mặc dù XGBoost là một thuật toán mạnh, nhưng việc tinh chỉnh tham số (Hyperparameter tuning) để mô hình không bị Overfitting trên tập train và khái quát hóa tốt trên tập test là một quá trình lặp đi lặp lại phức tạp.\n",
    "    * **Tổng hợp báo cáo:** Việc tổng hợp kết quả từ code, biểu đồ và insight thành một báo cáo mạch lạc, logic để giải thích cho các Câu hỏi nghiên cứu (1 & 2) đòi hỏi tư duy phân tích sâu sắc.\n",
    "\n",
    "* **Bài học rút ra:**\n",
    "    * **Tầm quan trọng của Data Cleaning:** Qua việc phân tích biến Categorical và Missing, em nhận thấy khâu làm sạch dữ liệu quyết định phần lớn độ chính xác của mô hình. Một mô hình XGBoost tốt không thể cứu vãn một bộ dữ liệu bẩn.\n",
    "    * **Insight về thị trường:** Khi trả lời câu hỏi nghiên cứu số 1 (Địa lý) và số 2 (Tiện nghi), em học được cách sử dụng dữ liệu để xác nhận các giả định thực tế: vị trí và các tiện ích \"cốt lõi\" (như máy lạnh, WC riêng) là những yếu tố ảnh hưởng trọng yếu nhất đến giá phòng.\n",
    "    * **Kỹ năng Technical & Soft skills:** Nâng cao kỹ năng lập trình Python (thư viện Scikit-learn, XGBoost), kỹ năng trực quan hóa dữ liệu và kỹ năng viết báo cáo khoa học. Đồng thời, học được cách quản lý version code khi làm việc nhóm để tránh conflict với các thành viên khác.\n",
    "\n",
    "* **Nếu có thêm thời gian:**\n",
    "    * Cải thiện bộ crawl data để chạy đa luồng (multithreading), giúp thu thập dữ liệu nhanh hơn và mở rộng sang các trang web khác để so sánh.\n",
    "    * Thử nghiệm các kỹ thuật Imputation nâng cao hơn (như dùng KNN hoặc MICE) để xử lý missing values thay vì các phương pháp thống kê cơ bản.\n",
    "    * Đào sâu hơn vào Feature Importance của XGBoost để loại bỏ các biến nhiễu, giúp mô hình nhẹ hơn nhưng vẫn giữ được độ chính xác cao."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "073c9f39",
   "metadata": {},
   "source": [
    "### Thành viên Châu Huỳnh Phúc - 23120329\n",
    "\n",
    "* **Khó khăn gặp phải:**\n",
    "    * **Thách thức về kỹ thuật dữ liệu:** Với vai trò chịu trách nhiệm chính về Processing và Feature Engineering, khó khăn lớn nhất là việc xử lý các dữ liệu thô từ thực tế (website phongtro123.com) thường không nhất quán, chứa nhiều giá trị thiếu và sai lệch. Việc biến đổi các đặc trưng thô thành các biến có ý nghĩa cho mô hình học máy đòi hỏi sự tỉ mỉ và logic cao.\n",
    "    * **Phân tích tương quan phức tạp:** Khi thực hiện ma trận tương quan, việc xác định được đâu là mối liên hệ thực sự có giá trị giữa hàng chục biến số để rút ra insight không hề dễ dàng, đòi hỏi phải quan sát dữ liệu dưới nhiều góc độ khác nhau.\n",
    "    * **Áp lực quản lý:** Việc vừa thực hiện chuyên môn, vừa phải quản lý tiến độ, kiểm soát deadline và báo cáo trạng thái dự án cho cả nhóm đòi hỏi khả năng bao quát và xử lý tình huống linh hoạt để đảm bảo không thành viên nào bị chậm tiến độ.\n",
    "\n",
    "* **Cách vượt qua:**\n",
    "    * **Quy trình hóa công việc:** Áp dụng nghiêm ngặt quy trình CRISP-DM để chia nhỏ các giai đoạn xử lý, giúp việc kiểm soát dữ liệu từ khâu làm sạch đến Feature Engineering trở nên có hệ thống hơn.\n",
    "    * **Tận dụng sức mạnh nhóm:** Sử dụng cơ chế Peer Review (kiểm tra chéo) để các thành viên khác thẩm định lại logic xử lý dữ liệu của mình, giúp phát hiện sớm các lỗi sai sót trong mã nguồn.\n",
    "    * **Công cụ quản lý hiện đại:** Sử dụng triệt để Google Docs, Notion, ... để cập nhật trạng thái hàng tuần và GitHub để quản lý các phiên bản code, giúp việc theo dõi tiến độ của các thành viên (như việc crawl data theo các khoảng ID) trở nên minh bạch và dễ dàng điều chỉnh.\n",
    "\n",
    "* **Điều học được:**\n",
    "    * **Kỹ năng Feature Engineering:** Học được cách trích xuất và tối ưu hóa các đặc trưng quan trọng từ dữ liệu phòng trọ để nâng cao hiệu suất cho các mô hình như XGBoost hay CatBoost mà nhóm đã sử dụng.\n",
    "    * **Tư duy quản trị dự án:** Hiểu được tầm quan trọng của việc lập kế hoạch chi tiết (Milestones) và cách duy trì động lực cho nhóm thông qua việc giao tiếp liên tục trên Zalo và Google Meet.\n",
    "    * **Phân tích chuyên sâu:** Biết cách sử dụng ma trận tương quan và các kỹ thuật thống kê để bóc tách quy luật của thị trường, thay vì chỉ nhìn vào các con số bề nổi.\n",
    "\n",
    "* **Điều bất ngờ:**\n",
    "    * **Sức mạnh của yếu tố thời gian:** Em thực sự bất ngờ khi phân tích câu hỏi số 5 về yếu tố mùa vụ. Sự biến động giá thuê phòng trọ tại TP.HCM theo các cột mốc thời gian trong năm có quy luật rõ ràng hơn nhiều so với dự đoán ban đầu của nhóm.\n",
    "    * **Sự phức tạp của dữ liệu thực:** Dữ liệu thực tế khác xa với các bộ dữ liệu mẫu trong giáo trình; nó đòi hỏi sự can thiệp rất lớn của con người trong khâu tiền xử lý mới có thể đưa vào mô hình hóa.\n",
    "\n",
    "* **Hiểu biết mới về Data Science:**\n",
    "    * **Dữ liệu là cốt lõi:** Em rằng trong một dự án Khoa học dữ liệu, giai đoạn Tiền xử lý và Feature Engineering chiếm đến 80% công sức nhưng cũng chính là yếu tố quyết định sự thành bại của mô hình, chứ không chỉ là việc chọn thuật toán phức tạp.\n",
    "    * **Tính liên ngành:** Data Science không chỉ là lập trình hay toán học, mà còn cần sự am hiểu về lĩnh vực (domain knowledge) – cụ thể trong đồ án này là thị trường bất động sản cho thuê – để có thể đặt ra các câu hỏi nghiên cứu sắc bén và thực tế."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f628d43f",
   "metadata": {},
   "source": [
    "### Thành viên: Vũ Trần Phúc - 23120333\n",
    "\n",
    "* **Khó khăn gặp phải:**\n",
    "    * Giai đoạn thu thập dữ liệu: Dữ liệu thu thập được từ web thường không đồng nhất, các trường thông tin mô tả rải rác đòi hỏi phải xử lý kỹ lưỡng để phục vụ cho sau này.\n",
    "    * Preprocessing & EDA: Việc xử lý giá trị ngoại lai cho các biến số định lượng (giá, diện tích) rất nan giải do phân phối dữ liệu bị lệch hẳn về một phía. Rất khó để xác định ngưỡng cắt bỏ chính xác mà không vô tình loại bỏ các căn hộ cao cấp \"thật\" (giá cao do vị trí mặt tiền).\n",
    "    * Modeling & Tuning: Mô hình CatBoost tuy mạnh về biến phân loại nhưng lại có quá nhiều tham số phức tạp (độ sâu cây, hệ số điều chuẩn...). Việc tinh chỉnh để mô hình không bị overfitting trên tập huấn luyện tốn nhiều thời gian thử nghiệm mà kết quả kiểm thử đôi khi chưa cải thiện như kỳ vọng.\n",
    "\n",
    "* **Bài học rút ra:**\n",
    "    * Hiểu sâu về ngữ cảnh dữ liệu (Domain Knowledge) quan trọng không kém kỹ thuật code. Việc trực tiếp trả lời các câu hỏi nghiên cứu về biến động giá và vị trí giúp tôi hiểu rõ đặc thù thị trường, từ đó hỗ trợ tốt hơn cho quá trình xây dựng mô hình.\n",
    "    * Làm việc nhóm hiệu quả qua GitHub giúp tôi rèn luyện tính cẩn thận khi giải quyết xung đột mã nguồn . Tôi học hỏi được tư duy viết code sạch (clean code) từ bạn Khánh Hưng và cách xử lý dữ liệu thiếu logic từ bạn Quốc Khánh.\n",
    "    * Tầm quan trọng của kể chuyện với dữ liệu : Một mô hình chạy tốt cần đi kèm với một báo cáo súc tích, dễ hiểu. Kỹ năng trình bày kết quả là cầu nối quan trọng để đưa kỹ thuật đến gần hơn với thực tế.\n",
    "\n",
    "\n",
    "* **Nếu có thêm thời gian:**\n",
    "    * Phân tích địa lý trực quan hơn bằng cách sử dụng thư viện bản đồ để vẽ bản đồ nhiệt (Heatmap) cho TP.HCM, giúp người xem dễ dàng nhận diện vùng giá nóng/lạnh thay vì chỉ nhìn số liệu thống kê.\n",
    "    * Xây dựng hệ thống gợi ý bên cạnh bài toán dự đoán giá, giúp đề xuất các phòng trọ phù hợp dựa trên độ tương đồng về nhu cầu của người dùng."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
