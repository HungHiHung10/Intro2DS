{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6c3b0db7",
   "metadata": {},
   "source": [
    "# **Reflection**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00da70f8",
   "metadata": {},
   "source": [
    "### Thành viên: Nguyễn Hữu Khánh Hưng - 23120271\n",
    "\n",
    "- **Khó khăn gặp phải:**  \n",
    "  - Giai đoạn thu thập dữ liệu: Việc crawl từ phongtro123.com gặp nhiều trở ngại như rate limit (bị chặn request sau một số lượng lớn truy cập), dữ liệu không đồng nhất (nhiều bài đăng thiếu tiện ích hoặc mô tả text lộn xộn), và phải xử lý pagination thủ công để thu thập đủ >20.000 records mà không bị block IP.  \n",
    "  - Preprocessing & EDA: Dữ liệu thô có nhiều missing values (đặc biệt ở tiện ích như máy giặt, gác lửng), giá và diện tích cần regex phức tạp để chuyển về float, đồng thời phát hiện outliers (giá phòng \"ảo\" quá cao/thấp do lỗi nhập liệu hoặc đàm phán).  \n",
    "  - Modeling & Tuning: Xây dựng baseline Linear Regression khá đơn giản, nhưng khi tinh chỉnh hyperparameter cho XGBoost và Linear Regression Optimized by ElasticNet (grid/random search với learning_rate, max_depth, n_estimators...) tốn rất nhiều thời gian và tài nguyên tính toán (máy cá nhân chạy chậm, phải thử nhiều lần để tránh overfitting/underfitting).  \n",
    "  - Merge code & README: Khi hợp nhất code từ 4 thành viên, gặp conflict ở một số notebook như `Pre_Processing.ipynb`, phải refactor thủ công để code sạch và chạy ổn định trên mọi máy. Viết README chi tiết cũng mất kha khá thời gian để đảm bảo hướng dẫn tái hiện chính xác.\n",
    "\n",
    "- **Bài học rút ra:**  \n",
    "  - Preprocessing và feature engineering chiếm phần lớn thời gian (khoảng 60–70%) nhưng là yếu tố quyết định chất lượng mô hình – dữ liệu sạch giúp XGBoost đạt R² cao hơn hẳn Linear Regression.  \n",
    "  - Hyperparameter tuning không chỉ là \"thử nhiều\" mà cần chiến lược (bắt đầu từ learning_rate thấp + early stopping, dùng random search trước grid search để tiết kiệm thời gian).  \n",
    "  - Làm việc nhóm hiệu quả nhờ GitHub (branching, pull request, peer review) giúp tránh mất dữ liệu/code và học hỏi lẫn nhau (tôi học được cách xử lý categorical từ bạn Phạm Quốc Khánh, và word-embedding từ bạn Vũ Trần Phúc).  \n",
    "  - README và documentation rõ ràng rất quan trọng – không chỉ để giảng viên chấm mà còn giúp chính mình tái hiện dự án sau này.\n",
    "\n",
    "- **Nếu có thêm thời gian:**  \n",
    "  - Scale dataset lớn hơn bằng cách crawl thêm từ nhatot.com hoặc chotot.com để tăng độ đa dạng dữ liệu và giảm bias.  \n",
    "  - Thử nghiệm ensemble (stacking XGBoost + CatBoost + LightGBM) hoặc thêm Deep learning (MLP với PyTorch) để đẩy R² lên cao hơn.  \n",
    "  - Tích hợp thêm nhiều feature địa lý hơn (khoảng cách đến trường đại học/metro bằng Google Maps API) và deploy mô hình thành web app đơn giản (Streamlit) để dự đoán giá realtime cho sinh viên.  \n",
    "  - Thực hiện A/B testing hoặc cross-validation nâng cao hơn để đánh giá độ ổn định mô hình trên các phân khúc giá khác nhau (phòng dưới 3 triệu vs trên 5 triệu)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff11945a",
   "metadata": {},
   "source": [
    "### Thành viên: Phạm Quốc Khánh - 23120283\n",
    "\n",
    "* **Khó khăn gặp phải:**\n",
    "    * **Thách thức về thu thập dữ liệu:** Việc viết code crawl data đối mặt với nhiều trở ngại kỹ thuật như cấu trúc HTML của các trang tin không đồng nhất (đặc biệt là phần mô tả tiện ích) và cơ chế chặn request (rate limit) từ server, đòi hỏi phải xử lý ngoại lệ (exception handling) liên tục để đảm bảo thu thập đủ dữ liệu mà không bị ngắt quãng.\n",
    "    * **Xử lý dữ liệu thiếu (Missing Values):** Khi phân tích các biến Categorical (như loại phòng, tiện ích), em gặp khó khăn lớn trong việc ra quyết định điền giá trị khuyết. Việc chọn phương pháp điền (Imputation) sao cho hợp lý về mặt thực tế mà không làm sai lệch phân phối dữ liệu gốc là một bài toán cân não.\n",
    "    * **Tinh chỉnh mô hình phức tạp:** Mô hình XGBoost tuy mạnh nhưng có quá nhiều siêu tham số (hyperparameters). Việc tìm ra bộ tham số tối ưu thông qua Grid Search tốn rất nhiều thời gian tính toán và tài nguyên phần cứng, đôi khi dẫn đến tình trạng máy bị treo hoặc quá nhiệt.\n",
    "\n",
    "* **Cách vượt qua:**\n",
    "    * **Tối ưu hóa mã nguồn:** Em đã chia nhỏ quy trình crawl thành các batch nhỏ và lưu trữ trung gian (checkpoint), giúp việc resume lại quá trình thu thập dễ dàng hơn khi gặp lỗi mạng hoặc bị chặn IP.\n",
    "    * **Phân tích kỹ lưỡng trước khi xử lý:** Đối với các missing values, em đã thực hiện vẽ biểu đồ phân phối để so sánh trước và sau khi điền dữ liệu, đồng thời tham khảo ý kiến nhóm để thống nhất các quy tắc business logic (ví dụ: nếu diện tích < 15m2 thì khả năng cao không có máy giặt riêng).\n",
    "    * **Chiến lược Tuning hợp lý:** Thay vì chạy Grid Search vét cạn ngay từ đầu, em chuyển sang dùng Randomized Search để khoanh vùng tham số tốt trước, sau đó mới tinh chỉnh chi tiết, giúp tiết kiệm đáng kể thời gian training.\n",
    "\n",
    "* **Điều học được:**\n",
    "    * **Kỹ năng Data Cleaning chuyên sâu:** Học được cách sử dụng Regex và các thư viện Pandas thành thạo để \"làm sạch\" những dữ liệu văn bản lộn xộn, biến chúng thành các đặc trưng (features) sạch sẽ cho mô hình.\n",
    "    * **Tư duy định hướng dữ liệu (Data-driven):** Qua việc trả lời câu hỏi nghiên cứu số 1 (Địa lý) và số 2 (Tiện ích), em học được cách dùng số liệu để chứng minh hoặc bác bỏ các giả định cảm tính về giá nhà, ví dụ như mức độ ảnh hưởng thực sự của vị trí so với nội thất.\n",
    "    * **Làm chủ thuật toán Boosting:** Hiểu sâu hơn về cơ chế hoạt động của Gradient Boosting và cách các tham số như learning_rate hay max_depth ảnh hưởng trực tiếp đến độ phức tạp của model.\n",
    "\n",
    "* **Điều bất ngờ:**\n",
    "    * **Sự thống trị của vị trí địa lý:** Khi phân tích câu hỏi số 1, em khá bất ngờ khi thấy biến số về khu vực/quận huyện có tác động áp đảo lên giá phòng, lấn át hầu hết các tiện ích nội thất khác – điều này khẳng định câu nói \"Location, Location, Location\" trong bất động sản.\n",
    "    * **Hiệu năng của XGBoost:** Khả năng xử lý dữ liệu phi tuyến tính của XGBoost tốt hơn nhiều so với kỳ vọng ban đầu của em, đặc biệt là khi so sánh với các model cơ bản, nó bắt được các pattern phức tạp của dữ liệu giá phòng rất tốt.\n",
    "\n",
    "* **Hiểu biết mới về Data Science:**\n",
    "    * **Garbage In, Garbage Out:** Em thấm thía rằng dù thuật toán có xịn đến đâu (như XGBoost), nếu dữ liệu đầu vào (đặc biệt là các biến Categorical và Missing values) không được xử lý kỹ lưỡng thì kết quả dự đoán cũng sẽ vô nghĩa. Giai đoạn làm sạch dữ liệu thực sự là \"linh hồn\" của dự án.\n",
    "    * **Vai trò của trực quan hóa:** Các con số thống kê khô khan chỉ thực sự có ý nghĩa và dễ thuyết phục người khác (như trong báo cáo cuối kỳ) khi chúng được kể lại thông qua các biểu đồ trực quan, sinh động."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "073c9f39",
   "metadata": {},
   "source": [
    "### Thành viên Châu Huỳnh Phúc - 23120329\n",
    "\n",
    "* **Khó khăn gặp phải:**\n",
    "    * **Thách thức về kỹ thuật dữ liệu:** Với vai trò chịu trách nhiệm chính về Processing và Feature Engineering, khó khăn lớn nhất là việc xử lý các dữ liệu thô từ thực tế (website phongtro123.com) thường không nhất quán, chứa nhiều giá trị thiếu và sai lệch. Việc biến đổi các đặc trưng thô thành các biến có ý nghĩa cho mô hình học máy đòi hỏi sự tỉ mỉ và logic cao.\n",
    "    * **Phân tích tương quan phức tạp:** Khi thực hiện ma trận tương quan, việc xác định được đâu là mối liên hệ thực sự có giá trị giữa hàng chục biến số để rút ra insight không hề dễ dàng, đòi hỏi phải quan sát dữ liệu dưới nhiều góc độ khác nhau.\n",
    "    * **Áp lực quản lý:** Việc vừa thực hiện chuyên môn, vừa phải quản lý tiến độ, kiểm soát deadline và báo cáo trạng thái dự án cho cả nhóm đòi hỏi khả năng bao quát và xử lý tình huống linh hoạt để đảm bảo không thành viên nào bị chậm tiến độ.\n",
    "\n",
    "* **Cách vượt qua:**\n",
    "    * **Quy trình hóa công việc:** Áp dụng nghiêm ngặt quy trình CRISP-DM để chia nhỏ các giai đoạn xử lý, giúp việc kiểm soát dữ liệu từ khâu làm sạch đến Feature Engineering trở nên có hệ thống hơn.\n",
    "    * **Tận dụng sức mạnh nhóm:** Sử dụng cơ chế Peer Review (kiểm tra chéo) để các thành viên khác thẩm định lại logic xử lý dữ liệu của mình, giúp phát hiện sớm các lỗi sai sót trong mã nguồn.\n",
    "    * **Công cụ quản lý hiện đại:** Sử dụng triệt để Google Docs, Notion, ... để cập nhật trạng thái hàng tuần và GitHub để quản lý các phiên bản code, giúp việc theo dõi tiến độ của các thành viên (như việc crawl data theo các khoảng ID) trở nên minh bạch và dễ dàng điều chỉnh.\n",
    "\n",
    "* **Điều học được:**\n",
    "    * **Kỹ năng Feature Engineering:** Học được cách trích xuất và tối ưu hóa các đặc trưng quan trọng từ dữ liệu phòng trọ để nâng cao hiệu suất cho các mô hình như XGBoost hay CatBoost mà nhóm đã sử dụng.\n",
    "    * **Tư duy quản trị dự án:** Hiểu được tầm quan trọng của việc lập kế hoạch chi tiết (Milestones) và cách duy trì động lực cho nhóm thông qua việc giao tiếp liên tục trên Zalo và Google Meet.\n",
    "    * **Phân tích chuyên sâu:** Biết cách sử dụng ma trận tương quan và các kỹ thuật thống kê để bóc tách quy luật của thị trường, thay vì chỉ nhìn vào các con số bề nổi.\n",
    "\n",
    "* **Điều bất ngờ:**\n",
    "    * **Sức mạnh của yếu tố thời gian:** Em thực sự bất ngờ khi phân tích câu hỏi số 5 về yếu tố mùa vụ. Sự biến động giá thuê phòng trọ tại TP.HCM theo các cột mốc thời gian trong năm có quy luật rõ ràng hơn nhiều so với dự đoán ban đầu của nhóm.\n",
    "    * **Sự phức tạp của dữ liệu thực:** Dữ liệu thực tế khác xa với các bộ dữ liệu mẫu trong giáo trình; nó đòi hỏi sự can thiệp rất lớn của con người trong khâu tiền xử lý mới có thể đưa vào mô hình hóa.\n",
    "\n",
    "* **Hiểu biết mới về Data Science:**\n",
    "    * **Dữ liệu là cốt lõi:** Em rằng trong một dự án Khoa học dữ liệu, giai đoạn Tiền xử lý và Feature Engineering chiếm đến 80% công sức nhưng cũng chính là yếu tố quyết định sự thành bại của mô hình, chứ không chỉ là việc chọn thuật toán phức tạp.\n",
    "    * **Tính liên ngành:** Data Science không chỉ là lập trình hay toán học, mà còn cần sự am hiểu về lĩnh vực (domain knowledge) – cụ thể trong đồ án này là thị trường bất động sản cho thuê – để có thể đặt ra các câu hỏi nghiên cứu sắc bén và thực tế."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f628d43f",
   "metadata": {},
   "source": [
    "### Thành viên Vũ Trần Phúc – 23120333\n",
    "\n",
    "* **Khó khăn gặp phải:**\n",
    "\n",
    "  * **Biểu diễn ngữ nghĩa cho dữ liệu mô tả:** Thách thức lớn trong quá trình làm việc với dữ liệu văn bản là việc lựa chọn và đánh giá cách biểu diễn ngữ nghĩa phù hợp cho các mô tả phòng trọ. Không phải mọi thông tin trong văn bản đều đóng góp tích cực cho bài toán dự đoán giá, nên cần cân nhắc giữa việc giữ lại nội dung quan trọng và tránh làm tăng nhiễu cho mô hình.\n",
    "  * **Phân tích Numerical và Outliers:** Dữ liệu số như giá thuê, diện tích, số lượng tiện ích có phân phối lệch và xuất hiện nhiều outliers bất thường. Việc xác định đâu là outliers hợp lý về mặt thực tế thị trường, đâu là lỗi dữ liệu không đơn thuần chỉ dựa vào thống kê mà cần kết hợp với hiểu biết domain.\n",
    "  * **Diễn giải câu hỏi nghiên cứu:** Khi trả lời các câu hỏi nghiên cứu về mối quan hệ giữa biến động giá và vị trí (Câu 3, 4), thách thức nằm ở việc chuyển các kết quả phân tích định lượng thành các lập luận logic, dễ hiểu và có giá trị thực tiễn.\n",
    "\n",
    "* **Cách vượt qua:**\n",
    "\n",
    "  * **Thử nghiệm và đánh giá nhiều phương án embedding:** Tiến hành so sánh các cách biểu diễn văn bản khác nhau  từ đó lựa chọn phương án Word Embedding mang lại hiệu quả tốt nhất và có ảnh hưởng tích cực đến các chỉ số đánh giá.\n",
    "  * **Kết hợp thống kê và trực quan hóa:** Sử dụng boxplot, IQR và phân phối histogram để phát hiện outliers, đồng thời đối chiếu với bối cảnh thực tế của thị trường thuê phòng nhằm tránh loại bỏ các giá trị có ý nghĩa.\n",
    "  * **Phối hợp nhóm để kiểm chứng kết luận:** Trao đổi với các thành viên khác về cách diễn giải kết quả phân tích vị trí và giá, từ đó tinh chỉnh lập luận và đảm bảo tính nhất quán trong toàn bộ báo cáo.\n",
    "\n",
    "* **Điều học được:**\n",
    "\n",
    "  * **Ứng dụng Word Embedding trong bài toán thực tế:** Hiểu rõ cách biến dữ liệu văn bản phi cấu trúc thành các đặc trưng số có thể sử dụng hiệu quả trong mô hình học máy.\n",
    "  * **Phân tích dữ liệu Numerical chuyên sâu:** Nâng cao kỹ năng nhận diện và xử lý outliers, cũng như đánh giá ảnh hưởng của chúng đến kết quả mô hình và phân tích.\n",
    "  * **Xây dựng mô hình CatBoost:** Học được cách cấu hình, huấn luyện và đánh giá mô hình CatBoost cho dữ liệu hỗn hợp (Numerical + Categorical), cũng như vai trò của tuning tham số đối với hiệu suất mô hình.\n",
    "\n",
    "* **Điều bất ngờ:**\n",
    "\n",
    "  * **Tác động mạnh của vị trí đến giá thuê:** Khi phân tích câu hỏi nghiên cứu, em bất ngờ trước mức độ ảnh hưởng rõ rệt của yếu tố vị trí (quận, khu vực) đến biến động giá, đôi khi còn lớn hơn cả các đặc trưng vật lý như diện tích hay số tiện ích.\n",
    "  * **Giá trị của dữ liệu văn bản:** Các đặc trưng trích xuất từ mô tả phòng trọ sau khi embedding mang lại nhiều thông tin hơn dự đoán ban đầu và góp phần cải thiện kết quả mô hình.\n",
    "\n",
    "* **Hiểu biết mới về Data Science:**\n",
    "\n",
    "  * **Sự kết hợp đa dạng dữ liệu:** Một mô hình hiệu quả không chỉ dựa vào dữ liệu số mà còn cần khai thác tốt dữ liệu phi cấu trúc như văn bản để phản ánh đầy đủ thực tế.\n",
    "  * **Tầm quan trọng của diễn giải kết quả:** Data Science không dừng lại ở việc xây dựng mô hình có độ chính xác cao, mà còn ở khả năng giải thích kết quả và liên hệ chúng với các câu hỏi nghiên cứu và bối cảnh thực tế, đặc biệt trong báo cáo tổng kết dự án."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef85e3d5",
   "metadata": {},
   "source": [
    "\n",
    "* **Nếu có thêm thời gian:**\n",
    "    * Cải thiện bộ crawl data để chạy đa luồng (multithreading), giúp thu thập dữ liệu nhanh hơn và mở rộng sang các trang web khác để so sánh.\n",
    "    * Thử nghiệm các kỹ thuật Imputation nâng cao hơn (như dùng KNN hoặc MICE) để xử lý missing values thay vì các phương pháp thống kê cơ bản.\n",
    "    * Đào sâu hơn vào Feature Importance của XGBoost để loại bỏ các biến nhiễu, giúp mô hình nhẹ hơn nhưng vẫn giữ được độ chính xác cao."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
